{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3XzRci_wKpL"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DL-3fSEJwKpL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "\n",
        "# Stuff needed to stablish a public tunnel\n",
        "!pip install flask pyngrok\n",
        "from flask import Flask, request, jsonify, Response\n",
        "from pyngrok import ngrok\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqKu-IE1wKpL"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model\n",
        "Execute only if you want to run your pre-trained model"
      ],
      "metadata": {
        "id": "2HiQtgX1Aeay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_model_a100\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucSkHF42o_2K"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 1) Prompt template for LDAP tasks and end-of-sequence token\n",
        "ldap_prompt = \"\"\"Below is an instruction that describes how to handle an LDAP request in JSON format, paired with an example input. Ensure you preserve the exact JSON structure, key names, and types, and correctly close all opened braces `{{}}`, brackets `[]` and quotation marks.  Write a strictly valid LDAP response in JSON format, maintaining formatting and syntax.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# 2) Fixed instruction for all examples\n",
        "ingest_instruction = \"Generate a valid LDAP response in JSON format based on the given request, paying special attention to correctly closing every opened brace and bracket so that the output is syntactically complete JSON.\"\n",
        "\n",
        "# 3) Formatting function for columns 'input' and 'output'\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs  = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for inp, out in zip(inputs, outputs):\n",
        "        prompt = ldap_prompt.format(ingest_instruction, inp, out) + EOS_TOKEN\n",
        "        texts.append(prompt)\n",
        "    # Return a dict with key 'text' for the trainer to use as the sequence\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# 4) Load the CSV dataset, assuming headers 'input' and 'output'\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\"train\": \"/content/combined.csv\"},\n",
        "    split=\"train\",\n",
        "    delimiter=\";\"\n",
        ")\n",
        "\n",
        "# 5) Filter out rows without a response (NaN)\n",
        "dataset = dataset.filter(lambda x: x[\"output\"] is not None)\n",
        "\n",
        "# 6) Map examples to generate the 'text' field for the trainer\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX74Gn4WtjER"
      },
      "source": [
        "# L4\n",
        "Settings for L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jJuGMNgtk3B"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    packing=True,           # <- importante para acelerar si tus ejemplos son cortos\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_ratio = 0.12, # 10 % de warm-up dinámico\n",
        "        num_train_epochs = 6, # 3 pasadas completas\n",
        "        learning_rate = 5e-5, # LR más conservador\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_torch_fused\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        gradient_checkpointing = False,        # big VRAM saver on L4\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "        max_grad_norm = 0.5, # gradient clipping helps stability\n",
        "        bf16 = True,   # True If supports BF16, otherwise False. A100 Supports\n",
        "        dataset_kwargs = {\n",
        "        \"train_on_inputs\": False\n",
        "        }\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQAe5vh2bJwh"
      },
      "source": [
        "#A100\n",
        "Settings for A100 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6627f72f09ea4a31b878ef8c691c870e",
            "1fe9b44c83e946dca1e17643a2d756eb",
            "34ad3cd66ca24f51b1161ab3ad4a959d",
            "688e790227954078b57184885f17c7ca",
            "e520f600a7664b91bcb9748b37b0ff18",
            "5df6f1f95e1a409eaf7d696c58157583",
            "c8bdab4f539e4f3a99e0fe1a41520d9c",
            "e164cd9a180b4368b38b32bdcc94b44b",
            "2a900d5c1dbe4a7a9002cf1062b90f27",
            "20b6d456be4c4e9d943bc23c7a831c8f",
            "a00fd20e6008444ca4637dceebbaceab"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "9bdff28b-25a1-4256-f0b8-767d4c002a25"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6627f72f09ea4a31b878ef8c691c870e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/328 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = mod el,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 16,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_ratio = 0.12, # 10 % de warm-up dinámico\n",
        "        num_train_epochs = 6, # 3 pasadas completas\n",
        "        learning_rate = 5e-5, # LR más conservador\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "        max_grad_norm = 0.5, # gradient clipping helps stability\n",
        "        bf16 = True,   # True If supports BF16, otherwise False. A100 Supports\n",
        "        fp16 = False,  # True If not supports BF16, otherwise False\n",
        "        dataset_kwargs = {\n",
        "        \"train_on_inputs\": False\n",
        "        }\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "bcc33d85-c17d-4040-da03-f8aa2583cd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "7.135 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "This run the model and the ngrok app. Copy the URL that is displayed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eudFYdZgz6EN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from flask import Response\n",
        "\n",
        "# Replace with your ngrok token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok.set_auth_token(\"30NT1X2ROB17qn8yDgMTdYRmbqZ_7huvrkfzVr99UF7i5X874\")\n",
        "\n",
        "# Configuration variables\n",
        "BASE_DN = \"dc=ejemplo,dc=com\"\n",
        "COMPANY_TYPE = \"Bank\"\n",
        "LANGUAGE = \"English\"\n",
        "\n",
        "# Open ngrok tunnel on port 5000\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"🌐 Tunnel URL:\", public_url)\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "def build_ldap_prompt(input_message, manual: bool = False):\n",
        "    \"\"\"\n",
        "    Build the instruction+input prompt for the LLM.\n",
        "\n",
        "    manual=True  -> include explicit config (BASE_DN, COMPANY_TYPE, LANGUAGE).\n",
        "    manual=False -> ask the model to infer a single, consistent base DN and language from the request/domain context.\n",
        "\n",
        "    \"\"\"\n",
        "    if manual:\n",
        "        instruction = f\"\"\"\n",
        "You are a simulated LDAP server (honeypot).\n",
        "Use the following fixed context:\n",
        "  • Base DN: {BASE_DN}\n",
        "  • Company Type: {COMPANY_TYPE}\n",
        "  • Language: {LANGUAGE}\n",
        "\n",
        "Rules (always):\n",
        "  - Mirror the request \"messageID\" in EVERY object you emit.\n",
        "  - Keep JSON valid: balanced braces, double quotes for keys/strings.\n",
        "  - Keep attributes realistic and short. Keep DNs consistent within the same response.\n",
        "        \"\"\".strip()\n",
        "    else:\n",
        "        instruction = \"\"\"\n",
        "You are a simulated LDAP server (honeypot).\n",
        "\n",
        "AUTO mode – infer context from the request:\n",
        "  - Infer a single, realistic base DN (domain) and keep it consistent across ALL outputs:\n",
        "      * If protocolOp.searchRequest.baseObject is a non-empty DN -> reuse its domain.\n",
        "      * Else, if you see email-like strings (e.g., \"user@domain.tld\"), convert to a DN:\n",
        "          \"dc=domain,dc=tld\" (add more dc= parts if needed).\n",
        "      * Else, if any DN appears in filters or attributes, reuse its domain.\n",
        "      * Else, choose a neutral, plausible domain and be consistent (e.g., \"dc=example,dc=org\").\n",
        "  - Infer language for human-readable strings (e.g., diagnosticMessage):\n",
        "      * Prefer Spanish if baseObject or values suggest Spanish context (e.g., \"dc=es\",\n",
        "        OU/CN names in Spanish, or Spanish department names).\n",
        "      * Prefer English otherwise.\n",
        "      * Do not alter attribute values that are identifiers (DNs, mail, cn).\n",
        "\n",
        "Rules (always):\n",
        "  - Mirror the request \"messageID\" in EVERY object you emit.\n",
        "  - Keep JSON valid: balanced braces, double quotes for keys/strings.\n",
        "  - Keep attributes realistic and short. Keep DNs consistent within the same response.\n",
        "        \"\"\".strip()\n",
        "    return ldap_prompt.format(instruction, json.dumps(input_message, ensure_ascii=False), \"\")\n",
        "\n",
        "def budget_for_request(message: dict) -> int:\n",
        "    \"\"\"\n",
        "    Decide max_new_tokens from the incoming LDAP request.\n",
        "    Small, readable buckets. If nothing matches, return 1000.\n",
        "    \"\"\"\n",
        "    op = message.get(\"protocolOp\")\n",
        "    if not isinstance(op, dict):\n",
        "        return 1000\n",
        "\n",
        "    # --- Non-search ops (short replies) ---\n",
        "    if \"bindRequest\" in op:\n",
        "        return 160                      # one bindResponse\n",
        "    if \"abandonRequest\" in op:\n",
        "        return 48                       # we shouldn't emit anything anyway\n",
        "    if any(k in op for k in (\"addRequest\", \"modifyRequest\", \"modDNRequest\", \"delRequest\")):\n",
        "        return 320                      # small structured responses\n",
        "\n",
        "    # --- Search ops ---\n",
        "    if \"searchRequest\" in op:\n",
        "        sr = op[\"searchRequest\"]\n",
        "        scope = sr.get(\"scope\", None)   # 0 base, 1 singleLevel, 2 wholeSubtree\n",
        "\n",
        "        # sizeLimit\n",
        "        size_limit = sr.get(\"sizeLimit\", 0) or 0\n",
        "        try:\n",
        "            size_limit = int(size_limit)\n",
        "        except Exception:\n",
        "            size_limit = 0\n",
        "\n",
        "        # attributes count\n",
        "        attrs = sr.get(\"attributes\", [])\n",
        "        attr_count = len(attrs) if isinstance(attrs, list) else 0\n",
        "\n",
        "        # filter type\n",
        "        filt = sr.get(\"filter\", {})\n",
        "        filter_type = next(iter(filt.keys())) if isinstance(filt, dict) and filt else None\n",
        "\n",
        "        # Base budget: ~80 tokens per entry + 160 overhead\n",
        "        expected = min(size_limit, 10) if size_limit > 0 else 6\n",
        "        base = 160 + 80 * expected\n",
        "\n",
        "        # scope=0 (RootDSE/Subschema-like) → small\n",
        "        if scope == 0:\n",
        "            return max(256, min(base, 512))\n",
        "\n",
        "        # “wide” filters or many attributes tend to expand\n",
        "        if filter_type in (\"substrings\", \"present\") or attr_count > 5:\n",
        "            return 768 if (size_limit == 0 or size_limit >= 8) else max(512, min(base, 640))\n",
        "\n",
        "        # scope=1 (single level) moderate\n",
        "        if scope == 1:\n",
        "            return max(384, min(base, 640))\n",
        "\n",
        "        # scope=2 (whole subtree) default\n",
        "        if scope == 2:\n",
        "            return max(512, min(base, 768))\n",
        "\n",
        "        # unknown scope but still a search\n",
        "        return min(base, 768)\n",
        "\n",
        "    # --- Fallback for anything else ---\n",
        "    return 1000\n",
        "\n",
        "def run_inference(prompt_text, max_tokens: int):\n",
        "    \"\"\"Run the LLM and return raw decoded output.\"\"\"\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_tokens, use_cache=True)\n",
        "    return tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "def split_json_objects(raw: str) -> tuple[list[dict], bool]:\n",
        "    \"\"\"\n",
        "    Split a response with consecutive JSON objects, returning a\n",
        "    list of valid objects and partial=True if the queue ended\n",
        "    with an incomplete object\n",
        "    \"\"\"\n",
        "    dec = json.JSONDecoder()\n",
        "    s = raw.lstrip()\n",
        "    i, n = 0, len(s)\n",
        "    objs: list[dict] = []\n",
        "    partial = False\n",
        "    while i < n:\n",
        "        j = s.find('{', i)\n",
        "        if j == -1:\n",
        "            break\n",
        "        try:\n",
        "            obj, idx = dec.raw_decode(s, j)\n",
        "            if isinstance(obj, dict):\n",
        "                objs.append(obj)\n",
        "            i = idx\n",
        "        except json.JSONDecodeError:\n",
        "            partial = True     # we detected truncated tail\n",
        "            break\n",
        "    return objs, partial\n",
        "\n",
        "def extract_response(decoded_output):\n",
        "    \"\"\"\n",
        "    Extract the LLM's response from decoded output using indexes.\n",
        "    \"\"\"\n",
        "    start_tok = \"### Response:\"\n",
        "    start = decoded_output.find(start_tok)\n",
        "    if start == -1:\n",
        "        return decoded_output.strip()\n",
        "    start += len(start_tok)\n",
        "    end = decoded_output.find(\"<|end_of_text|>\", start)\n",
        "    if end == -1:\n",
        "        end = len(decoded_output)  # ← fallback seguro\n",
        "    return decoded_output[start:end].strip()\n",
        "\n",
        "def inference_ldap_message(message: dict):\n",
        "    \"\"\"Generate a response to the incoming LDAP message using the LLM.\"\"\"\n",
        "    prompt = build_ldap_prompt(message)\n",
        "    mx = budget_for_request(message)              # ← dynamic budget here\n",
        "    decoded = run_inference(prompt, max_tokens=mx)\n",
        "    return extract_response(decoded)\n",
        "\n",
        "@app.route(\"/receive_data\", methods=[\"POST\"])\n",
        "def recibir_datos():\n",
        "    data = request.get_json()\n",
        "    if not data:\n",
        "        return jsonify({\"error\": \"No input received\"}), 400\n",
        "\n",
        "    # If it's a JSON string (double-encoded), unbox once\n",
        "    if isinstance(data, str):\n",
        "        try:\n",
        "            data_raw = data\n",
        "            data = json.loads(data)\n",
        "        except json.JSONDecodeError:\n",
        "            return jsonify({\"error\": \"Body is a JSON string but not valid JSON inside.\"}), 400\n",
        "\n",
        "    if not isinstance(data, dict):\n",
        "        return jsonify({\"error\": \"Expected a JSON object at top-level.\"}), 400\n",
        "\n",
        "    print(\"📩 Received data:\", data_raw)\n",
        "    llm_raw = inference_ldap_message(data)\n",
        "    print(\"📤 LLM raw response:\", llm_raw)\n",
        "\n",
        "    # === NEW: split into individual JSON objects ===\n",
        "    try:\n",
        "        entries, partial = split_json_objects(llm_raw)\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": \"Parsing error\", \"detail\": str(e)}), 500\n",
        "\n",
        "    # If the response was truncated and it was a searchRequest, we guarantee Done\n",
        "    op = data.get(\"protocolOp\")\n",
        "    is_search = isinstance(op, dict) and \"searchRequest\" in op\n",
        "\n",
        "    if partial and is_search:\n",
        "      # Is there already a complete Done?\n",
        "      has_done = any(\n",
        "          isinstance(e, dict)\n",
        "          and isinstance(e.get(\"protocolOp\"), dict)\n",
        "          and \"searchResDone\" in e[\"protocolOp\"]\n",
        "          for e in entries\n",
        "      )\n",
        "      if not has_done:\n",
        "          print(\"⚠️ Injected searchResDone due to truncated tail\")\n",
        "          mid = int(data.get(\"messageID\", 0))\n",
        "          entries.append({\n",
        "              \"messageID\": mid,\n",
        "              \"protocolOp\": {\n",
        "                  \"searchResDone\": {\n",
        "                      \"resultCode\": 0,\n",
        "                      \"matchedDN\": \"\",\n",
        "                      \"diagnosticMessage\": \"\"\n",
        "                  }\n",
        "              }\n",
        "          })\n",
        "\n",
        "    # Stream each entry back on its own line\n",
        "    def generate():\n",
        "        for entry in entries:\n",
        "            chunk = json.dumps(entry, ensure_ascii=False)\n",
        "            print(\"🚚 Sending chunk:\", chunk)\n",
        "            yield chunk + \"\\n\"\n",
        "\n",
        "    return Response(generate(), mimetype=\"application/json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "fe21ff78-43c7-4023-ba31-f203ec2c11e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model_a100/tokenizer_config.json',\n",
              " 'lora_model_a100/special_tokens_map.json',\n",
              " 'lora_model_a100/tokenizer.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model_a100\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model_a100\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1fe9b44c83e946dca1e17643a2d756eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5df6f1f95e1a409eaf7d696c58157583",
            "placeholder": "​",
            "style": "IPY_MODEL_c8bdab4f539e4f3a99e0fe1a41520d9c",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "20b6d456be4c4e9d943bc23c7a831c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a900d5c1dbe4a7a9002cf1062b90f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34ad3cd66ca24f51b1161ab3ad4a959d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e164cd9a180b4368b38b32bdcc94b44b",
            "max": 328,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a900d5c1dbe4a7a9002cf1062b90f27",
            "value": 328
          }
        },
        "5df6f1f95e1a409eaf7d696c58157583": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6627f72f09ea4a31b878ef8c691c870e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fe9b44c83e946dca1e17643a2d756eb",
              "IPY_MODEL_34ad3cd66ca24f51b1161ab3ad4a959d",
              "IPY_MODEL_688e790227954078b57184885f17c7ca"
            ],
            "layout": "IPY_MODEL_e520f600a7664b91bcb9748b37b0ff18"
          }
        },
        "688e790227954078b57184885f17c7ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20b6d456be4c4e9d943bc23c7a831c8f",
            "placeholder": "​",
            "style": "IPY_MODEL_a00fd20e6008444ca4637dceebbaceab",
            "value": " 328/328 [00:01&lt;00:00, 245.92 examples/s]"
          }
        },
        "a00fd20e6008444ca4637dceebbaceab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8bdab4f539e4f3a99e0fe1a41520d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e164cd9a180b4368b38b32bdcc94b44b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e520f600a7664b91bcb9748b37b0ff18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}